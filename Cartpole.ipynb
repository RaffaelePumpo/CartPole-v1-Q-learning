{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d69bcf",
   "metadata": {},
   "source": [
    "# CartPole-v1 with Q-learning made by  [Raffaele Pumpo](https://github.com/RaffaelePumpo) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3ca2b",
   "metadata": {},
   "source": [
    "The Cartpole problem is a classic control problem in the field of reinforcement learning. It involves a pole that is attached to a cart, and the objective is to balance the pole upright on the cart by moving the cart left or right. The system is considered solved when the pole is balanced for a certain amount of time, or a certain number of time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a427b3",
   "metadata": {},
   "source": [
    "### Tools used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c263d",
   "metadata": {},
   "source": [
    "For the following algorithm, I have used \"anaconda3\", installed gym with:\n",
    "\n",
    "*pip install gym* (gym 0.17.3) **Use this version the function used might be different for output and input**\n",
    "\n",
    "*pip install -U scikit-learn*\n",
    "\n",
    "*pip install typing*\n",
    "\n",
    "in the prompt of the anaconda 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e85d1b",
   "metadata": {},
   "source": [
    "**Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "befdfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "import gym\n",
    "import time , random , math\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441aaaf",
   "metadata": {},
   "source": [
    "**Build the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa1aee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431bd20",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7dd8f",
   "metadata": {},
   "source": [
    "Q-Learning is a model-free, off-policy reinforcement learning algorithm. It is used to learn the optimal policy for a given Markov Decision Process (MDP) by estimating the optimal action-value function (also known as the Q-function). The Q-function represents the expected reward for taking a specific action in a specific state and following the optimal policy thereafter. The Q-learning algorithm updates the Q-function estimates iteratively based on observed state-action transitions and received rewards. The goal of Q-learning is to find the policy that maximizes the expected cumulative reward. Once the Q-function has converged, the algorithm can be used to determine the optimal policy by selecting the action with the highest Q-value for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8500b",
   "metadata": {},
   "source": [
    "**Convert the continuous space in discrete one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "613e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6 ,12)\n",
    "lower_bounds = [env.observation_space.low[2],-math.radians(50)]\n",
    "upper_bounds = [env.observation_space.high[2],math.radians(50)]\n",
    "\n",
    "def discretizer(_,__, angle, pole_velocity) -> Tuple[int, ...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int, est.transform([[angle, pole_velocity]])[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5c9ce",
   "metadata": {},
   "source": [
    "**Initialise the Q value table with zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9cc7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197854b",
   "metadata": {},
   "source": [
    "**Create a policy function using Q-table and  greedly selecting the highest Q value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1531b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy ( state : tuple):\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46285636",
   "metadata": {},
   "source": [
    "**Update the values of the table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "437c51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value ( reward : float , state_new : tuple , discount_factor =1) ->float:\n",
    "    future_optimal_value = np.max(Q_table[state_new])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a7018",
   "metadata": {},
   "source": [
    "**Define the rates** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e3d38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate ( n : int , min_rate = 0.1) ->float:\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "\n",
    "def exploration_rate ( n : int , min_rate = 0.1) ->float:\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4ffa5",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cbf4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert the number of episodes that you want to train the cart, suggest at least 500:100\n",
      "Episode: 0 Score: 18.0\n",
      "Episode: 1 Score: 29.0\n",
      "Episode: 2 Score: 12.0\n",
      "Episode: 3 Score: 16.0\n",
      "Episode: 4 Score: 23.0\n",
      "Episode: 5 Score: 31.0\n",
      "Episode: 6 Score: 38.0\n",
      "Episode: 7 Score: 22.0\n",
      "Episode: 8 Score: 18.0\n",
      "Episode: 9 Score: 59.0\n",
      "Episode: 10 Score: 31.0\n",
      "Episode: 11 Score: 27.0\n",
      "Episode: 12 Score: 18.0\n",
      "Episode: 13 Score: 25.0\n",
      "Episode: 14 Score: 17.0\n",
      "Episode: 15 Score: 23.0\n",
      "Episode: 16 Score: 17.0\n",
      "Episode: 17 Score: 20.0\n",
      "Episode: 18 Score: 20.0\n",
      "Episode: 19 Score: 12.0\n",
      "Episode: 20 Score: 29.0\n",
      "Episode: 21 Score: 22.0\n",
      "Episode: 22 Score: 13.0\n",
      "Episode: 23 Score: 19.0\n",
      "Episode: 24 Score: 21.0\n",
      "Episode: 25 Score: 15.0\n",
      "Episode: 26 Score: 20.0\n",
      "Episode: 27 Score: 21.0\n",
      "Episode: 28 Score: 35.0\n",
      "Episode: 29 Score: 14.0\n",
      "Episode: 30 Score: 13.0\n",
      "Episode: 31 Score: 49.0\n",
      "Episode: 32 Score: 12.0\n",
      "Episode: 33 Score: 12.0\n",
      "Episode: 34 Score: 18.0\n",
      "Episode: 35 Score: 24.0\n",
      "Episode: 36 Score: 19.0\n",
      "Episode: 37 Score: 12.0\n",
      "Episode: 38 Score: 20.0\n",
      "Episode: 39 Score: 18.0\n",
      "Episode: 40 Score: 36.0\n",
      "Episode: 41 Score: 41.0\n",
      "Episode: 42 Score: 43.0\n",
      "Episode: 43 Score: 17.0\n",
      "Episode: 44 Score: 17.0\n",
      "Episode: 45 Score: 76.0\n",
      "Episode: 46 Score: 39.0\n",
      "Episode: 47 Score: 26.0\n",
      "Episode: 48 Score: 18.0\n",
      "Episode: 49 Score: 74.0\n",
      "Episode: 50 Score: 15.0\n",
      "Episode: 51 Score: 69.0\n",
      "Episode: 52 Score: 27.0\n",
      "Episode: 53 Score: 42.0\n",
      "Episode: 54 Score: 31.0\n",
      "Episode: 55 Score: 22.0\n",
      "Episode: 56 Score: 48.0\n",
      "Episode: 57 Score: 65.0\n",
      "Episode: 58 Score: 91.0\n",
      "Episode: 59 Score: 122.0\n",
      "Episode: 60 Score: 154.0\n",
      "Episode: 61 Score: 25.0\n",
      "Episode: 62 Score: 12.0\n",
      "Episode: 63 Score: 21.0\n",
      "Episode: 64 Score: 34.0\n",
      "Episode: 65 Score: 32.0\n",
      "Episode: 66 Score: 20.0\n",
      "Episode: 67 Score: 22.0\n",
      "Episode: 68 Score: 13.0\n",
      "Episode: 69 Score: 34.0\n",
      "Episode: 70 Score: 22.0\n",
      "Episode: 71 Score: 10.0\n",
      "Episode: 72 Score: 40.0\n",
      "Episode: 73 Score: 28.0\n",
      "Episode: 74 Score: 50.0\n",
      "Episode: 75 Score: 53.0\n",
      "Episode: 76 Score: 43.0\n",
      "Episode: 77 Score: 57.0\n",
      "Episode: 78 Score: 20.0\n",
      "Episode: 79 Score: 14.0\n",
      "Episode: 80 Score: 90.0\n",
      "Episode: 81 Score: 25.0\n",
      "Episode: 82 Score: 142.0\n",
      "Episode: 83 Score: 39.0\n",
      "Episode: 84 Score: 145.0\n",
      "Episode: 85 Score: 30.0\n",
      "Episode: 86 Score: 72.0\n",
      "Episode: 87 Score: 22.0\n",
      "Episode: 88 Score: 32.0\n",
      "Episode: 89 Score: 15.0\n",
      "Episode: 90 Score: 33.0\n",
      "Episode: 91 Score: 20.0\n",
      "Episode: 92 Score: 65.0\n",
      "Episode: 93 Score: 58.0\n",
      "Episode: 94 Score: 116.0\n",
      "Episode: 95 Score: 48.0\n",
      "Episode: 96 Score: 75.0\n",
      "Episode: 97 Score: 28.0\n",
      "Episode: 98 Score: 47.0\n",
      "Episode: 99 Score: 90.0\n"
     ]
    }
   ],
   "source": [
    "# Set the number of episodes to run\n",
    "n_episodes = input(\"Insert the number of episodes that you want to train the cart, suggest at least 500:\")\n",
    "# Convert in int\n",
    "n_episodes = int(n_episodes)\n",
    "# Loop over each episode\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    # Reset the environment and discretize the current state\n",
    "    current_state, done = discretizer(*env.reset()), False\n",
    "\n",
    "    # Initialize the episode score to 0\n",
    "    score = 0\n",
    "\n",
    "    # Loop until the episode is done\n",
    "    while not done:\n",
    "\n",
    "        # Choose an action based on the current policy\n",
    "        action = policy(current_state)\n",
    "\n",
    "        # With probability epsilon, choose a random action instead\n",
    "        if random.random() < exploration_rate(episode):\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take a step in the environment with the chosen action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Discretize the new state\n",
    "        new_state = discretizer(*obs)\n",
    "\n",
    "        # Update the Q table with the new information\n",
    "        lr = learning_rate(episode)\n",
    "        learned_value = new_Q_value(reward, new_state)\n",
    "        old_value = Q_table[current_state + (action,)]\n",
    "        Q_table[current_state + (action,)] = old_value * (1 - lr) + lr * learned_value\n",
    "\n",
    "        # Update the current state and the episode score\n",
    "        current_state = new_state\n",
    "        score += reward\n",
    "\n",
    "        # Render the environment every 5 episodes\n",
    "        # avoid seeing each episode of training\n",
    "        if episode % 5 == 0:\n",
    "            env.render()\n",
    "\n",
    "    # Print the episode number and score\n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cfe15",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bfc",
   "metadata": {},
   "source": [
    "Observing the simulation, we can conclude that the algorithm just implemented It's useful for the CartPole. Initially the pole falls in a very short time, as the following video:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9d724",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/RaffaelePumpo/CartPole-v1-Q-learning/blob/main/Initial.gif?raw=true\" alt=\"drawing\" width=\"600\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9c2aa",
   "metadata": {},
   "source": [
    "After different episodes, the algorithm allows to move the cart in a such a way that the pole is balanced for a certain time, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabea584",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/RaffaelePumpo/CartPole-v1-Q-learning/blob/main/Initial.gif?raw=true\" alt=\"drawing\" width=\"600\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8c7eb",
   "metadata": {},
   "source": [
    "We can see also that the algorithm is correct, observing the values of the score for each episode, the score is greater when the pole doesn't fall for a longer time and smaller otherwise. The score for the last episode is greater with respect to the initial ones thanks to the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
